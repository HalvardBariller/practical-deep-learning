{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87e2e074",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j"
   },
   "source": [
    "# Koopman operator learning, a toy case: Duffing oscillator\n",
    "\n",
    "**-- PLEASE SUBMIT YOUR COMPLETED NOTEBOOK WITH CELL OUTPUTS --**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00165f9",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j"
   },
   "source": [
    "\n",
    "The aim of this notebook is to describe the dynamics of a non-linear dynamical system by means of the Koopman theory.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We consider a quantity $x$ (a vector) which evolves with time, following a dynamical system. Think for example of the joint location of the planets in our solar system, which follows the law of gravitation.\n",
    "\n",
    "Formally, given an initial state $x(t=0) \\in \\mathbb{R}^n$ at time $t=0$, the time evolution of $x$ is governed by the following dynamical system:\n",
    "$$\n",
    "\\dot{x}(t) = f(x(t)) \\quad \\text(1)\n",
    "$$\n",
    "\n",
    "where $\\dot{x}(t) := \\frac{dx(t)}{dt}$ is the temporal derivative, and $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ is a given map describing the dynamics.\n",
    "\n",
    "For a given $f$, it is not always possible to solve the differential equation (1) analytically. For this reason, instead, numerical schemes are usually employed, to integrate in time $t$ the equation (1), so as to propagate the initial condition $x(0)$ up to a desired time $T$; think of $x(T) = x(0) + \\int_{t=0}^{T} f(x(t)) dt$. The discretization in time of eq (1) or of the integral introduces numerical approximations, and yields estimates of $x(T)$ of various quality depending on the discretization scheme.\n",
    "\n",
    "In the field of numerical simulations, discretization schemes have been studied for a long time, and numerical solvers already exist to provide good estimates of integrals (far better than with the naive discretization $x_{k+\\delta} = x_k + \\delta\\,f(x_k)$ for a discrete time increment $\\delta$, which induces a $O(\\delta^2)$ error at each time step).\n",
    "\n",
    "The goal of this practical session is to make use of such numerical solvers to improve the learning of dynamical systems with neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e1a1ed",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:03.783685Z",
     "start_time": "2024-03-11T08:18:02.231161Z"
    },
    "id": "bde86020"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "\n",
    "# arrange the dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e878e38",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "id": "mDyFzROl3b-N"
   },
   "source": [
    "## Duffing oscillator\n",
    "\n",
    "As a toy example, we consider the Duffing oscillator, where the state $x = (x_1, x_2) \\in \\mathbb{R}^2$ follows the dynamical system described by the following ODEs:\n",
    "\n",
    "$$\n",
    "\\dot{x}_1 = x_2\\\\\n",
    "\\dot{x}_2 = x_1 - x_1^3\n",
    "$$\n",
    "\n",
    "To integrate in time the ODEs, a 4th order Runge-Kutta scheme can be used. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd0920",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:03.787910Z",
     "start_time": "2024-03-11T08:18:03.785158Z"
    },
    "id": "9c76fa87"
   },
   "outputs": [],
   "source": [
    "def duffing(array_x: np.ndarray) -> np.ndarray:\n",
    "    array_dx = np.zeros(array_x.shape)\n",
    "    array_dx[0] = array_x[1]\n",
    "    array_dx[1] = array_x[0] - array_x[0] ** 3\n",
    "    return array_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaded1f",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:06.731549Z",
     "start_time": "2024-03-11T08:18:03.788501Z"
    },
    "id": "8140314e"
   },
   "outputs": [],
   "source": [
    "t_max = 500  # Time-horizon integration\n",
    "n_iter = 5000  # Number of time steps integration\n",
    "n_initial_conditions = 60  # Number of initial conditions\n",
    "\n",
    "dim_system = 2\n",
    "\n",
    "# Generate initial conditions\n",
    "matrix_x0 = (np.random.rand(n_initial_conditions, dim_system) - 0.5) * 4\n",
    "array_t = np.linspace(0, t_max, n_iter)\n",
    "array3d_xt = np.zeros((matrix_x0.shape[0], matrix_x0.shape[1], n_iter))\n",
    "\n",
    "for i in tqdm(range(matrix_x0.shape[0])):\n",
    "    # Lambda function is used as solve_ivp requires a function of the form f(t, x)\n",
    "    ode_result = solve_ivp(lambda _t, array_x: duffing(array_x),\n",
    "                           [0, t_max],\n",
    "                           matrix_x0[i],\n",
    "                           method='RK45',\n",
    "                           t_eval=array_t)\n",
    "\n",
    "    array3d_xt[i, :] = ode_result.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a60a58f",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j"
   },
   "source": [
    "The following plot shows trajectories for different initial conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7518bbca",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:07.058800Z",
     "start_time": "2024-03-11T08:18:06.733503Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "id": "ea6b6ae1",
    "outputId": "e30d6069-609b-4faf-8db4-3f891cf2599b"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 5))\n",
    "ax = fig.add_subplot(131)\n",
    "cm = plt.get_cmap(\"tab10\")\n",
    "print(cm)\n",
    "for i in range(10):\n",
    "    ax.plot(array3d_xt[i, 0, :], array3d_xt[i, 1, :], lw=0.5, color=cm(i))\n",
    "    ax.plot(array3d_xt[i, 0, 0], array3d_xt[i, 1, 0], 'o', lw=1.5, color=cm(i))  #initial condition\n",
    "ax.set_xlabel('$x_1$', fontsize=20)\n",
    "ax.set_ylabel('$x_2$', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfbbe4b",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "id": "oSzWSnIR6_WD"
   },
   "source": [
    "## The Koopman operator\n",
    "### Discontinuous in time case\n",
    "Given the discrete non-linear dynamical system\n",
    "\n",
    "$$\n",
    "x_{k+1} = F(x_k)\n",
    "$$\n",
    "\n",
    "where $F$ might be the $\\delta$-discretised flow map of the continuous dynamical system in eq (1) given by\n",
    "\n",
    "$$\n",
    "x_{k+1} = F(x_k) := x_k + \\int_{k}^{k + \\delta} f(x(s))ds \n",
    "$$\n",
    "\n",
    "and $X = (x_k)_{k = 0}^N$ the discrete time series of the system state.\n",
    "\n",
    "\n",
    "The _Koopman_ theory states that there exists an infinite-dimensional linear operator $\\mathcal{K}$ that advances in time all observable functions $(g_i)_{i = 1}^m$ given by $g_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}$\n",
    "\n",
    "$$\n",
    "\\mathcal{K} g_i(x) = g_i \\circ F(x)\n",
    "$$\n",
    "\n",
    "This way, the non-linear dynamics of $x$, described by $F$, can be turned into a **linear** dynamical system, described by $\\mathcal{K}$, acting on another representation space, formed by the observable quantities $g_i(x)$.\n",
    "\n",
    "Indeed, let $g_i$ be an observable function and denoting ${g_i}_k := g_i(x_k)$, using the previous equation, the time evolution of the observables is given by\n",
    "\n",
    "$$\n",
    "{g_i}_{k+1} = g_i(x_{k+1}) = g_i(F(x_k)) = g_i \\circ F(x_k) = \\mathcal{K} g_i(x_k) = \\mathcal{K} {g_i}_k\n",
    "$$\n",
    "\n",
    "then, the linearised dynamics of the observables is given by the following equation\n",
    "\n",
    "$$\n",
    "{g_i}_{k+1} = \\mathcal{K} {g_i}_k\n",
    "$$\n",
    "\n",
    "\n",
    "It is then sufficient to find a function $g: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ with $m \\gg n$ that embeds the state $x$ into a \"larger enough\" dimensional space $m$ such that the linear operator $\\mathcal{K}$ can be inferred by a matrix $\\mathbf{K} \\in \\mathbb{R}^{m \\times m}$.\n",
    "\n",
    "To project back the dynamics from the Koopman space ($\\mathbb{R}^m$, where $g(x)$ lives) to the phase space ($\\mathbb{R}^n$, where $x$ lives), a supplementary function $\\varphi: \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$ is needed. Going from $x$ to the Koopman space and back yields $\\varphi \\text{ o  } g = $ Id.\n",
    "\n",
    "Under this condition, the functions $g$, $\\varphi$ and $\\mathbf{K}$ can be parametrized $g_{\\theta}$, $\\varphi_{\\rho}$ and $\\mathbf{K}_{\\phi}$, and the parameters $\\theta$, $\\rho$ and $\\phi$ can be learned minimizing suitable loss functions. \n",
    "\n",
    "For this purpose, given a time series $X = \\{x_k | k = 1 \\ldots N \\}$, the following conditions hold:\n",
    "\n",
    "\n",
    "1.   Reconstruction error\n",
    "     $$\n",
    "     \\Vert \\varphi_\\rho (g_\\theta(x_k)) - x_k  \\Vert = 0\n",
    "     $$\n",
    "2.   Prediction error in Koopman space\n",
    "     $$\n",
    "     \\Vert \\mathbf{K_{\\phi}} g_{\\theta} ( x_k ) - g_{\\theta} (x_{k+1})  \\Vert = 0\n",
    "     $$\n",
    "3.   Prediction error in the phase space\n",
    "     $$\n",
    "     \\Vert \\varphi_{\\rho} \\left( \\mathbf{K_{\\phi}} g_{\\theta} ( x_k )\\right) - x_{k+1} \\Vert = 0\n",
    "     $$\n",
    "\n",
    "The last three errors can be used as loss functions to train three different neural networks. These different neural networks compose our architecture that can be summarized as in the following sketch:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09d2833",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j"
   },
   "source": [
    "![architecture](./architecture.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151e7941",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:07.082473Z",
     "start_time": "2024-03-11T08:18:07.059630Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Km6f2WruJPf7",
    "outputId": "47f394f2-8e94-478a-f938-304119baa739"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Flatten the trajectories w.r.t. initial conditions \n",
    "# and only keep data in the form of (dim_system, n_iter * n_initial_conditions)\n",
    "matrix_x_data = array3d_xt[:, :, :-1].swapaxes(0, 1).reshape(2, -1).T\n",
    "matrix_x_next_data = array3d_xt[:, :, 1:].swapaxes(0, 1).reshape(2, -1).T\n",
    "\n",
    "(matrix_x_data_train,\n",
    " matrix_x_data_test,\n",
    " matrix_x_next_data_train,\n",
    " matrix_x_next_data_test) = train_test_split(matrix_x_data,\n",
    "                                             matrix_x_next_data,\n",
    "                                             test_size=0.2)\n",
    "\n",
    "# Cast type to float32\n",
    "matrix_x_data_train = matrix_x_data_train.astype(np.float32)\n",
    "matrix_x_data_test = matrix_x_data_test.astype(np.float32)\n",
    "matrix_x_next_data_train = matrix_x_next_data_train.astype(np.float32)\n",
    "matrix_x_next_data_test = matrix_x_next_data_test.astype(np.float32)\n",
    "\n",
    "print(matrix_x_data_train.shape,\n",
    "      matrix_x_data_test.shape,\n",
    "      matrix_x_next_data_train.shape,\n",
    "      matrix_x_next_data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a4a31",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:08.749418Z",
     "start_time": "2024-03-11T08:18:07.083557Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390f726b",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:08.758588Z",
     "start_time": "2024-03-11T08:18:08.751090Z"
    },
    "id": "kipqu2hvJ2cB"
   },
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "# torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "\n",
    "batch_size = 2000  # data per batch\n",
    "\n",
    "tensor2d_x_data_train = torch.from_numpy(matrix_x_data_train).to(device)\n",
    "tensor2d_x_next_data_train = torch.from_numpy(matrix_x_next_data_train).to(device)\n",
    "tensor2d_x_data_test = torch.from_numpy(matrix_x_data_test).to(device)\n",
    "tensor2d_x_next_data_test = torch.from_numpy(matrix_x_next_data_test).to(device)\n",
    "\n",
    "torch_dataset_train = TensorDataset(tensor2d_x_data_train,\n",
    "                                    tensor2d_x_next_data_train)\n",
    "\n",
    "torch_dataset_test = TensorDataset(tensor2d_x_data_test,\n",
    "                                   tensor2d_x_next_data_test)\n",
    "\n",
    "train_dataloader = DataLoader(torch_dataset_train,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "test_dataloader = DataLoader(torch_dataset_test,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=True)\n",
    "\n",
    "# create the models\n",
    "feature_dim = 2  # dimension of the Duffing oscillator\n",
    "hidden_layer = 5  # number of hidden layers in g (ENCODER) and \\varphi (DECODER) \n",
    "output_dim = 30  # dimension in Koopman space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6cd7c",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:08.764348Z",
     "start_time": "2024-03-11T08:18:08.760347Z"
    },
    "id": "nhSxlEg-KNwu"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, list_layer_dim: list):\n",
    "        super().__init__()\n",
    "        self.list_layer_dim = list_layer_dim\n",
    "        self.list_FC = nn.ModuleList()\n",
    "        for i in range(len(self.list_layer_dim) - 1):\n",
    "            input_dim = self.list_layer_dim[i]\n",
    "            output_dim = self.list_layer_dim[i + 1]\n",
    "            self.list_FC.append(nn.Linear(input_dim, output_dim))\n",
    "\n",
    "    def forward(self, tensor2d_x):\n",
    "        for i in range(len(self.list_layer_dim) - 2):\n",
    "            tensor2d_x = F.elu(self.list_FC[i](tensor2d_x))\n",
    "        return self.list_FC[-1](tensor2d_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cea7716",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:08.772916Z",
     "start_time": "2024-03-11T08:18:08.766836Z"
    },
    "id": "RjcLaquyKNxB"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, list_layer_dim: list):\n",
    "        super().__init__()\n",
    "        self.list_layer_dim = list_layer_dim\n",
    "        self.list_FC = nn.ModuleList()\n",
    "        for i in range(len(self.list_layer_dim) - 1, 0, -1):\n",
    "            input_dim = self.list_layer_dim[i]\n",
    "            output_dim = self.list_layer_dim[i - 1]\n",
    "            self.list_FC.append(nn.Linear(input_dim, output_dim))\n",
    "\n",
    "    def forward(self, tensor2d_x: torch.Tensor):\n",
    "        for i in range(len(self.list_layer_dim) - 2):\n",
    "            tensor2d_x = F.elu(self.list_FC[i](tensor2d_x))\n",
    "        return self.list_FC[-1](tensor2d_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd8ed80",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:08.782536Z",
     "start_time": "2024-03-11T08:18:08.776808Z"
    },
    "id": "qUOovEfQKNxB"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, feature_dim: int, hidden_layer: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        list_layer_dim = \\\n",
    "            [output_dim if i == hidden_layer\n",
    "             else feature_dim + i * (output_dim - feature_dim) // hidden_layer\n",
    "             for i in range(hidden_layer + 1)]\n",
    "        self.encoder = Encoder(list_layer_dim)\n",
    "        self.decoder = Decoder(list_layer_dim)\n",
    "\n",
    "    def forward(self, tensor2d_x: torch.Tensor):\n",
    "        tensor2d_x = self.Encoder(tensor2d_x)\n",
    "        return self.Decoder(tensor2d_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753dad9",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "id": "lOpkBElrKek6"
   },
   "source": [
    "The Koopman operator $\\mathbf{K}$ (which is linear, and thus a matrix) must have a [spectral radius](https://en.wikipedia.org/wiki/Spectral_radius) $\\rho(\\mathbf{K})\\le 1$. Such condition will provide a stable -or at least a marginally stable- Koopman operator. To fulfill this requirement, we might leverage on the Perron-Frobenius theorem. \n",
    "\n",
    "The Perron-Frobenius th. states: if $\\mathbf{K}$ is a $m \\times m$ positive matrix i.e. $k_{ij} > 0$ for $1 \\le i,j \\le m$, then the following inequality holds:\n",
    "\n",
    "$$\n",
    "\\min_i \\sum_j k_{ij} \\le \\rho(\\mathbf{K}) \\le \\max_i \\sum_j k_{ij}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee6f02e",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j"
   },
   "source": [
    "**Question 1.** : Complete the `KoopmanModule` class to enforce $\\rho(\\mathbf{K})\\le 1$, using the Perron-Frobenius theorem. Check that the initialization fulfills this property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d973a",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:08.792204Z",
     "start_time": "2024-03-11T08:18:08.784882Z"
    },
    "id": "HlkD2_wiM3jz"
   },
   "outputs": [],
   "source": [
    "class KoopmanOperator(nn.Module):\n",
    "    def __init__(self, koopman_operator_dim: int):\n",
    "        super().__init__()\n",
    "        self.koopman_operator_dim = koopman_operator_dim\n",
    "        # TODO: Complete the KoopmanOperator class\n",
    "\n",
    "    def forward(self, tensor2d_x: torch.Tensor):\n",
    "        # First dimension of tensor2d_x is the batch size\n",
    "        if tensor2d_x.shape[1] != self.koopman_operator_dim:\n",
    "            sys.exit(f'Wrong Input Features. Please use tensor'\n",
    "                     f' with {self.koopman_operator_dim} Input Features')\n",
    "        # TODO: Implement the forward pass\n",
    "\n",
    "\n",
    "dim_observable = 10\n",
    "koopman_operator = KoopmanOperator(dim_observable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f69809b",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j"
   },
   "outputs": [],
   "source": [
    "# TODO: Check the spectrum initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4429ece4",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:08.807261Z",
     "start_time": "2024-03-11T08:18:08.800557Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MaRampsGUThz",
    "outputId": "5f200617-955e-4f4d-a818-fb8081d91ee9"
   },
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(feature_dim, hidden_layer, output_dim).to(device)\n",
    "koopman_operator = KoopmanOperator(output_dim).to(device)\n",
    "print(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b2c14f",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:09.091055Z",
     "start_time": "2024-03-11T08:18:08.808215Z"
    },
    "id": "63eQ5P9wU0fp"
   },
   "outputs": [],
   "source": [
    "learning_rate_autoencoder = 0.0001\n",
    "learning_rate_koopman = 0.00001\n",
    "\n",
    "optimiser_autoencoder = torch.optim.Adam(autoencoder.parameters(),\n",
    "                                         lr=learning_rate_autoencoder)\n",
    "\n",
    "optimiser_koopman = torch.optim.Adam(koopman_operator.parameters(),\n",
    "                                     lr=learning_rate_koopman)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ac6ac",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j"
   },
   "source": [
    "**Question 2.** : Define a function to compute the loss to be minimized. It should at least include the 3 terms listed above:\n",
    "- Reconstruction error\n",
    "- Prediction error in the Koopman space\n",
    "- Prediction error in the phase space\n",
    "\n",
    "Because the different objectives outlined by these losses may compete, the training can be difficult. You may try different variations on these losses and comment your findings. In order to improve the training process, one can for instance:\n",
    "- Add a multiplicative factor in front of each loss component, to balance their importance; how the scales of different losses are related?\n",
    "- We can refine the loss acting upon the latent space, by using a variational autoencoder approach. This is similar to the Gaussian likelihood used in the first practical (TD1). We want the prediction in the latent space (i.e. the Koopman space) to be a normal distribution $\\mathcal{N}(0, 1)$ . Add a corresponding loss for the latent space. Difference to 0 mean and 1 standard deviation must be thus included in the loss;\n",
    "- Freeze the gradients of one part of the network, for instance the encoder, for one specific objective, using the [`requires_grad`](https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad_.html) property. For instance:\n",
    "```python\n",
    "criterion = nn.MSELoss()\n",
    "...\n",
    "# Compute one part loss_l of the total loss\n",
    "# First deactivate gradient computation for irrelevant parts of the architecture\n",
    "for p in autoencoder.encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "loss_l = criterion(pred, target)\n",
    "# Restore the gradient computation\n",
    "for p in autoencoder.encoder.parameters():\n",
    "    p.requires_grad = True\n",
    "...\n",
    "total_loss = loss_1 + ... + loss_l + ...\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e71187c",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j"
   },
   "source": [
    "# Implement the loss function here\n",
    "def loss(X_, Y_, X_recon, gX_, gY_, gY_pred, Y_pred):\n",
    "    # To Be Implemented\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06a1df",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:09.099140Z",
     "start_time": "2024-03-11T08:18:09.092712Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement the loss function here\n",
    "# HINT: See the training process below to identify the different components of the loss\n",
    "def loss_koopman(tensor2d_x: torch.Tensor,\n",
    "                 tensor2d_x_next: torch.Tensor,\n",
    "                 tensor2d_decoded_x: torch.Tensor,\n",
    "                 tensor2d_observable_next: torch.Tensor,\n",
    "                 tensor2d_koopman_observable_next: torch.Tensor,\n",
    "                 tensor2d_predict_x_next: torch.Tensor):\n",
    "\n",
    "    # TODO: Implement the loss function here\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7751cd4c",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j"
   },
   "source": [
    "\n",
    "1.   Reconstruction error\n",
    "$$\n",
    "\\Vert \\varphi_\\rho (g_\\theta(x_k)) - x_k  \\Vert = 0\n",
    "$$\n",
    "2.   Prediction error in Koopman space\n",
    "$$\n",
    "\\Vert \\mathbf{K_{\\phi}} g_{\\theta} ( x_k ) - g_{\\theta} (x_{k+1})  \\Vert = 0\n",
    "$$\n",
    "3.   Prediction error in the phase space\n",
    "$$\n",
    "\\Vert \\varphi_{\\rho} \\left( \\mathbf{K_{\\phi}} g_{\\theta} ( x_k )\\right) - x_{k+1} \\Vert = 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51962e1",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j"
   },
   "source": [
    "**Question 3.**: The following cell executes the training loop. You can modify it in order to display the different intermediate losses computed in the function `LOSS` above. How do they evolve in time? Justify your final choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa255b0b",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:27.156223Z",
     "start_time": "2024-03-11T08:18:09.100800Z"
    },
    "id": "kys9nMNJVplk"
   },
   "outputs": [],
   "source": [
    "n_batch = len(train_dataloader)\n",
    "n_epoch = 5  # To be tuned \n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    autoencoder.train()\n",
    "    koopman_operator.train()\n",
    "    total_train_loss = 0\n",
    "    total_loss1, total_loss2, total_loss3, total_loss4 = 0, 0, 0, 0\n",
    "    for tensor2d_batch_x, tensor2d_batch_x_next in train_dataloader:\n",
    "        tensor2d_batch_x = tensor2d_batch_x.to(device)\n",
    "        tensor2d_batch_x_next = tensor2d_batch_x_next.to(device)\n",
    "\n",
    "        optimiser_autoencoder.zero_grad()\n",
    "        optimiser_koopman.zero_grad()\n",
    "\n",
    "        tensor2d_observable = autoencoder.encoder(tensor2d_batch_x)\n",
    "        tensor2d_observable_next = autoencoder.encoder(tensor2d_batch_x_next)\n",
    "\n",
    "        tensor2d_decoded_x = autoencoder.decoder(tensor2d_observable)\n",
    "\n",
    "        tensor2d_koopman_observable_next = koopman_operator(tensor2d_observable)\n",
    "\n",
    "        tensor2d_predict_x_next = autoencoder.decoder(tensor2d_koopman_observable_next)\n",
    "\n",
    "        tensor_loss_val = loss_koopman(tensor2d_batch_x,\n",
    "                                       tensor2d_batch_x_next,\n",
    "                                       tensor2d_decoded_x,\n",
    "                                       tensor2d_observable_next,\n",
    "                                       tensor2d_koopman_observable_next,\n",
    "                                       tensor2d_predict_x_next)\n",
    "\n",
    "        tensor_loss_val.backward()\n",
    "        optimiser_autoencoder.step()\n",
    "        optimiser_koopman.step()\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            total_train_loss += tensor_loss_val.item()\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(epoch, total_train_loss)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        autoencoder.eval()\n",
    "        koopman_operator.eval()\n",
    "        tensor_loss_val = None\n",
    "        with torch.no_grad():\n",
    "            total_test_loss = 0\n",
    "            for tensor2d_batch_x, tensor2d_batch_x_next in test_dataloader:\n",
    "                tensor2d_batch_x = tensor2d_batch_x.to(device)\n",
    "                tensor2d_batch_x_next = tensor2d_batch_x_next.to(device)\n",
    "\n",
    "                tensor2d_observable = \\\n",
    "                    autoencoder.encoder(tensor2d_batch_x)\n",
    "                tensor2d_observable_next = \\\n",
    "                    autoencoder.encoder(tensor2d_batch_x_next)\n",
    "                tensor2d_decoded_x = \\\n",
    "                    autoencoder.decoder(tensor2d_observable)\n",
    "                tensor2d_koopman_observable_next = \\\n",
    "                    koopman_operator(tensor2d_observable)\n",
    "                tensor2d_predict_x_next = \\\n",
    "                    autoencoder.decoder(tensor2d_koopman_observable_next)\n",
    "\n",
    "                tensor_loss_val = loss_koopman(tensor2d_batch_x,\n",
    "                                               tensor2d_batch_x_next,\n",
    "                                               tensor2d_decoded_x,\n",
    "                                               tensor2d_observable_next,\n",
    "                                               tensor2d_koopman_observable_next,\n",
    "                                               tensor2d_predict_x_next)\n",
    "\n",
    "            total_test_loss += tensor_loss_val.item()\n",
    "            print('-' * 50, 'TEST', '-' * 50)\n",
    "            print(epoch, total_test_loss)\n",
    "            print('-' * 106)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e210f651",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j"
   },
   "outputs": [],
   "source": [
    "# TODO: Explain the loss dynamics and justify your design choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65f8caf",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j"
   },
   "source": [
    "#### Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02260d4",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j"
   },
   "source": [
    "**Question 4.** : We want to ensure the Koopman operator is stable. This can be verified by checking whether its spectral radius $\\rho(\\mathbf{K})\\le 1$. Plot the eigenvalues of the Koopman operator in order to verify the bound on its spectral radius. You can use the [`numpy.linalg.eig`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html) function to retrieve the eigenvalues of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3dedc5",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:27.280634Z",
     "start_time": "2024-03-11T08:18:27.277762Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Check Koopman stability and plot the eigen values of the Koopman operator against the unit circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea3d47f",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:27.506771Z",
     "start_time": "2024-03-11T08:18:27.282187Z"
    }
   },
   "outputs": [],
   "source": [
    "n_grid = 30\n",
    "x1_min, x1_max = -2, 2\n",
    "x2_min, x2_max = -2, 2\n",
    "\n",
    "array_x1 = np.linspace(x1_min, x1_max, n_grid, dtype=np.float32)\n",
    "array_x2 = np.linspace(x2_min, x2_max, n_grid, dtype=np.float32)\n",
    "matrix_grid_x1, matrix_grid_x2 = np.meshgrid(array_x1, array_x2)\n",
    "\n",
    "array3d_dynamics = np.zeros((n_grid, n_grid, 2), dtype=np.float32)\n",
    "\n",
    "for i in range(n_grid):\n",
    "    for j in range(n_grid):\n",
    "        x1 = matrix_grid_x1[i, j]\n",
    "        x2 = matrix_grid_x2[i, j]\n",
    "        array3d_dynamics[i, j, :] = duffing(np.array([x1, x2]))\n",
    "\n",
    "# Set evaluation mode\n",
    "autoencoder.eval()\n",
    "koopman_operator.eval()\n",
    "\n",
    "array3d_dynamics_pred = np.zeros((n_grid, n_grid, 2), dtype=np.float32)\n",
    "\n",
    "for i in range(n_grid):\n",
    "    for j in range(n_grid):\n",
    "        x1 = matrix_grid_x1[i, j]\n",
    "        x2 = matrix_grid_x2[i, j]\n",
    "        tensor2d_x = torch.tensor([[x1, x2]], dtype=torch.float32).to(device)\n",
    "        tensor2d_observable = autoencoder.encoder(tensor2d_x)\n",
    "        tensor2d_koopman_observable_next = koopman_operator(tensor2d_observable)\n",
    "        tensor2d_predict_x_next = autoencoder.decoder(tensor2d_koopman_observable_next)\n",
    "        array_x_next = tensor2d_predict_x_next.cpu().detach().numpy().ravel()\n",
    "\n",
    "        # Here we compute a discretised version of the derivative thanks to the Koopman operator\n",
    "        # and the learned encoder/decoder\n",
    "\n",
    "        # (x_{k+1} - x_k) / \\delta_t = f(x_k) is approximated by (f is duffing here)\n",
    "        # (Decod(K(Encod(x_k))) - x_k) / \\delta_t\n",
    "\n",
    "        delta_time = (t_max / n_iter)\n",
    "        array3d_dynamics_pred[i, j, :] = (array_x_next - [x1, x2]) / delta_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a207f2f1",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:27.978748Z",
     "start_time": "2024-03-11T08:18:27.510291Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax = fig.add_subplot(131)\n",
    "ax.quiver(matrix_grid_x1,\n",
    "          matrix_grid_x2,\n",
    "          array3d_dynamics[:, :, 0],\n",
    "          array3d_dynamics[:, :, 1], scale=10)\n",
    "ax.set_title('True')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "ax.quiver(matrix_grid_x1,\n",
    "          matrix_grid_x2,\n",
    "          array3d_dynamics_pred[:, :, 0],\n",
    "          array3d_dynamics_pred[:, :, 1], scale=10)\n",
    "\n",
    "ax.set_title('Prediction')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "\n",
    "# Compute the error\n",
    "matrix_error = np.linalg.norm(array3d_dynamics - array3d_dynamics_pred, axis=2)\n",
    "matrix_error_log = np.log10(matrix_error + 1e-10)\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "cp = ax.contourf(matrix_grid_x1,\n",
    "                 matrix_grid_x2,\n",
    "                 matrix_error_log)\n",
    "\n",
    "fig.colorbar(cp)\n",
    "ax.set_title('Error in log scale')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd36af",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "id": "Z_OP_wUKJOCM"
   },
   "source": [
    "### Continuous in time case\n",
    "\n",
    "Considering $x_k$ as the observation of a state at time $t = k \\delta$, and $x_{k+1}$ the state at time $t+ \\delta$, for $\\delta \\rightarrow 0$  it is also possible to define the continuous-time infinitesimal generator of the Koopman operator family as\n",
    "\n",
    "$$\n",
    "\\mathcal{L} g (x_k)  = \\lim_{\\delta \\rightarrow 0} \\frac{\\mathcal{K}g(x_k)- g(x_{k})}{\\delta} = \\frac{g \\circ F (x_k) -x_k}{\\delta}\n",
    "$$\n",
    "\n",
    "The pevious expression defines the Lie derivative, and for this reason $\\mathcal{L}$ is known as the Lie operator. $\\mathcal{L}$ describes the continuous dynamics of the observables in the Koopman space:\n",
    "\n",
    "$$\n",
    "\\dot{g} (x) = \\mathcal{L} g(x).\n",
    "$$\n",
    "\n",
    "The latter can be further expressed as:\n",
    "\n",
    "$$\n",
    "\\dot{g} (x(t)) = \\frac{dg(x)}{dt} = \\nabla_x g \\frac{dx}{dt} = \\nabla_x g \\cdot f(x) =\\mathcal{L} g(x).\n",
    "$$\n",
    "\n",
    "Given $g_{\\theta}$, $\\varphi_{\\rho}$ and $\\mathbf{L}_{\\phi}$ three parameterized functions, the following conditions hold:\n",
    "\n",
    "1.   Reconstruction error\n",
    "     $$\n",
    "     \\Vert \\varphi_\\rho (g_\\theta(x)) - x  \\Vert = 0\n",
    "     $$\n",
    "2.   Prediction error in Koopman space\n",
    "     $$\n",
    "     \\Vert \\mathbf{L_{\\phi}} g_{\\theta} ( x ) - \\nabla g_{\\theta} \\cdot f(x)  \\Vert = 0\n",
    "     $$\n",
    "3.   Prediction error in the phase space\n",
    "     $$\n",
    "     \\Vert \\varphi_{\\rho} \\left( \\mathbf{L_{\\phi}} g_{\\theta} ( x )\\right) - f(x) \\Vert = 0\n",
    "     $$\n",
    "\n",
    "**Important Remark: As long as the system $f$ is known, the three errors can be computed without data belonging to trajectories.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7abecd",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:28.423609Z",
     "start_time": "2024-03-11T08:18:27.979996Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dataset for continuous Koopman\n",
    "# with the same amount of points of the Discontinuous Koopman case\n",
    "# But here no need to have continuous trajectories\n",
    "matrix_x0 = (np.random.rand(n_initial_conditions * (n_iter - 1), 2) - 0.5) * 4\n",
    "matrix_system_derivative_data = np.zeros(matrix_x0.shape)\n",
    "for i in tqdm(range(matrix_x0.shape[0])):\n",
    "    matrix_system_derivative_data[i, :] = duffing(matrix_x0[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d2845b",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:28.692682Z",
     "start_time": "2024-03-11T08:18:28.425694Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.quiver(matrix_x0[::50, 0],\n",
    "          matrix_x0[::50, 1],\n",
    "          matrix_system_derivative_data[::50, 0] * 0.2,\n",
    "          matrix_system_derivative_data[::50, 1] * 0.2, scale=10)\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d24a8b2",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:28.696747Z",
     "start_time": "2024-03-11T08:18:28.693686Z"
    }
   },
   "outputs": [],
   "source": [
    "# create the models\n",
    "feature_dim = 2  # dimension of the Duffing oscillator\n",
    "hidden_layer = 5  # number of hidden layers in g (ENCODER) and \\varphi (DECODER) \n",
    "output_dim = 30  # dimension in Koopman space\n",
    "batch_size = 2000  # data per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87b8aae",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:28.730955Z",
     "start_time": "2024-03-11T08:18:28.698193Z"
    }
   },
   "outputs": [],
   "source": [
    "(matrix_x_data_train,\n",
    " matrix_x_data_test,\n",
    " matrix_x_next_data_train,\n",
    " matrix_x_next_data_test) = train_test_split(matrix_x0,\n",
    "                                             matrix_system_derivative_data,\n",
    "                                             test_size=0.2)\n",
    "\n",
    "# Cast type to float32\n",
    "matrix_x_data_train = matrix_x_data_train.astype(np.float32)\n",
    "matrix_x_data_test = matrix_x_data_test.astype(np.float32)\n",
    "matrix_x_next_data_train = matrix_x_next_data_train.astype(np.float32)\n",
    "matrix_x_next_data_test = matrix_x_next_data_test.astype(np.float32)\n",
    "\n",
    "print(matrix_x_data_train.shape,\n",
    "      matrix_x_data_test.shape,\n",
    "      matrix_x_next_data_train.shape,\n",
    "      matrix_x_next_data_test.shape)\n",
    "\n",
    "torch_dataset_train = TensorDataset(torch.from_numpy(matrix_x_data_train),\n",
    "                                    torch.from_numpy(matrix_x_next_data_train))\n",
    "torch_dataset_test = TensorDataset(torch.from_numpy(matrix_x_data_test),\n",
    "                                   torch.from_numpy(matrix_x_next_data_test))\n",
    "\n",
    "train_dataloader = DataLoader(torch_dataset_train,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "test_dataloader = DataLoader(torch_dataset_test,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee7aec2",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:28.737993Z",
     "start_time": "2024-03-11T08:18:28.732552Z"
    },
    "id": "4254fe4c"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, list_layer_dim):\n",
    "        super().__init__()\n",
    "        self.list_layer_dim = list_layer_dim\n",
    "        self.list_FC = nn.ModuleList()\n",
    "        for i in range(len(self.list_layer_dim) - 1):\n",
    "            dim_input = self.list_layer_dim[i]\n",
    "            dim_output = self.list_layer_dim[i + 1]\n",
    "            self.list_FC.append(nn.Linear(dim_input, dim_output))\n",
    "\n",
    "    def forward(self, tensor2d_x):\n",
    "        for i in range(len(self.list_layer_dim) - 2):\n",
    "            tensor2d_x = F.elu(self.list_FC[i](tensor2d_x))\n",
    "        return self.list_FC[-1](tensor2d_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f976a4",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:28.743782Z",
     "start_time": "2024-03-11T08:18:28.739059Z"
    },
    "id": "c65379b3"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, list_layer_dim):\n",
    "        super().__init__()\n",
    "        self.list_layer_dim = list_layer_dim\n",
    "        self.list_FC = nn.ModuleList()\n",
    "        for i in range(len(self.list_layer_dim) - 1, 0, -1):\n",
    "            dim_input = self.list_layer_dim[i]\n",
    "            dim_output = self.list_layer_dim[i - 1]\n",
    "            self.list_FC.append(nn.Linear(dim_input, dim_output))\n",
    "\n",
    "    def forward(self, tensor2d_x):\n",
    "        for i in range(len(self.list_layer_dim) - 2):\n",
    "            tensor2d_x = F.elu(self.list_FC[i](tensor2d_x))\n",
    "        return self.list_FC[-1](tensor2d_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a5514",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:28.749986Z",
     "start_time": "2024-03-11T08:18:28.746520Z"
    },
    "id": "9ba2bab2"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, feature_dim, hidden_layer, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        list_layer_dim = \\\n",
    "            [output_dim if i == hidden_layer\n",
    "             else feature_dim + i * (output_dim - feature_dim) // hidden_layer\n",
    "             for i in range(hidden_layer + 1)]\n",
    "\n",
    "        self.encoder = Encoder(list_layer_dim)\n",
    "        self.decoder = Decoder(list_layer_dim)\n",
    "\n",
    "    def forward(self, tensor2d_x: torch.Tensor):\n",
    "        tensor2d_x = self.encoder(tensor2d_x)\n",
    "        return self.decoder(tensor2d_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be34124e",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "id": "Idmf_0fQEEst"
   },
   "source": [
    "The Lie operator must be defined such that it will be always stable by construction.\n",
    "To do that, we consider a matrix of parameters $\\Psi \\in \\mathbb{R}^{m \\times m}$ and a vector of parameters $\\Gamma \\in \\mathbb{R}^m$. The resulting Lie operator will be of the form:\n",
    "\n",
    "$$\n",
    "\\mathbf{L} = (\\Psi - \\Psi^T) - \\text{diag}(\\vert \\Gamma \\vert)\n",
    "$$\n",
    "\n",
    "with eigenvalues whose real part $\\Re(\\lambda) \\leq 0$ .\n",
    "See https://math.stackexchange.com/questions/952233/eigenvalues-of-the-sum-of-a-diagonal-matrix-and-a-skew-symmetric-matrix for the mathematical proof (identify the matrix). Moreover if $\\lambda \\in \\mathbb{C}$ is an eigenvalue of $\\mathbf{L}$, it turns out that its real part $\\Re(\\lambda) \\propto \\Vert \\Gamma \\Vert$, i.e. it only depends on $\\Gamma$.\n",
    "\n",
    "Remark: $- \\text{diag}(\\vert \\Gamma \\vert)$ is always a diagonal matrix with non-positive elements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf883413",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j"
   },
   "source": [
    "**Question 4.** : As you did for the discrete case, you now have to implement the `LieModule` module. It should have the form indicated above to guarantee $\\Re(\\lambda) \\leq 0$. Check that the initialization fulfills this property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c54423",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:28.755986Z",
     "start_time": "2024-03-11T08:18:28.750831Z"
    }
   },
   "outputs": [],
   "source": [
    "class LieModule(nn.Module):\n",
    "    def __init__(self, lie_operator_dim: int):\n",
    "        super().__init__()\n",
    "        self.lie_operator_dim = lie_operator_dim\n",
    "        # TODO: Complete function\n",
    "        \n",
    "        \n",
    "    def forward(self, tensor2d_x: torch.Tensor):\n",
    "        if tensor2d_x.shape[1] != self.lie_operator_dim:\n",
    "            sys.exit(f'Wrong Input Features. Please use tensor'\n",
    "                     f' with {self.koopman_operator_dim} Input Features')\n",
    "\n",
    "        # TODO: Implement forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a8100",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:28.763050Z",
     "start_time": "2024-03-11T08:18:28.756846Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53d38a89",
    "outputId": "125c7078-30ba-4359-aee4-a32ce2983ba0"
   },
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(feature_dim, hidden_layer, output_dim).to(device)\n",
    "lie_operator = LieModule(output_dim).to(device)\n",
    "print(autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd165652",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "id": "dZDn5-VsiLBk"
   },
   "source": [
    "Some tricks are needed to train. If the autoencoder and the Lie model are learned at the same speed, the training turns out to be highly unstable since the three loss functions have moving targets. For this reason, the Lie learning rate has been chosen smaller than the autoencoder one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe4a6d8",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:28.768610Z",
     "start_time": "2024-03-11T08:18:28.764129Z"
    },
    "id": "ee4be8b4"
   },
   "outputs": [],
   "source": [
    "learning_rate_autoencoder = 0.0001\n",
    "learning_rate_lie = 0.00001\n",
    "\n",
    "optimiser_autoencoder = torch.optim.Adam(autoencoder.parameters(),\n",
    "                                         lr=learning_rate_autoencoder,\n",
    "                                         weight_decay=1e-3)\n",
    "optimiser_lie = torch.optim.Adam(lie_operator.parameters(),\n",
    "                                 lr=learning_rate_lie,\n",
    "                                 weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b864cd",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "id": "Wj5k6s5QizG8"
   },
   "source": [
    "A further loss is considered to stabilize the learning stage. The state $x$ belongs to a compact set, since it is the solution of a dissipative dynamical system. This is not true for $g(x)$ (we need to choose appropriate activation functions to have appropriate Liptchitz guarantees). To avoid discrepancies in magnitudes of $g_i(x)$, a regularization loss is added:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{m} \\sum_m g_i(x) = 0  \\quad \\text{and} \\quad  \\sigma = \\left( \\frac{1}{m}\\sum_m(g_i(x)-\\mu)^2 \\right)^{1/2} = 1\n",
    "$$\n",
    "\n",
    "inspired by VAE.\n",
    "\n",
    "For the training to be smooth, the encoder parameters are not affected by the **prediction loss in phase space**. This is based on an empirical observation and is motivated by the fact that the encoder appears in the three losses and plays a competitive role against the decoder and the Lie model. This should not affect the results since the encoder remains coupled with the decoder in the **reconstruction loss** and with the Lie operator in the **prediction loss in Koopman space**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc484a1",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j"
   },
   "source": [
    "**Question 5.** : Implement the loss function similarly to what you did for the **Question 2.** Note that here you should use the dynamics $f$ and its values for a set of points belonging to the domain $\\left[ -2, 2 \\right]^2$ while no data from proper **trajectories** are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3150e19c",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:28.775679Z",
     "start_time": "2024-03-11T08:18:28.769873Z"
    }
   },
   "outputs": [],
   "source": [
    "# Implement the loss function here\n",
    "# See the training process below to identify the different components of the loss\n",
    "def loss(tensor2d_x: torch.Tensor,\n",
    "         tensor2d_x_next: torch.Tensor,\n",
    "         tensor2d_decoded_x: torch.Tensor,\n",
    "         tensor2d_observable_next: torch.Tensor,\n",
    "         tensor2d_lie_observable_next: torch.Tensor,\n",
    "         tensor2d_predict_x_next: torch.Tensor,\n",
    "         tensor2d_jvp: torch.Tensor):\n",
    "\n",
    "    # TODO: Implement loss\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c8e6a",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "id": "QEiVJGHWmFa9"
   },
   "source": [
    "**Since trajectories are not needed**, random states can be sampled from the system manifold $x_1 \\in [-2, 2]$, $x_2 \\in [-2, 2]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e994b",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:42.649579Z",
     "start_time": "2024-03-11T08:18:28.776771Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a4d1ff9",
    "outputId": "da2793c0-f28d-4cad-9435-302d9fe4ed50"
   },
   "outputs": [],
   "source": [
    "n_batch = len(train_dataloader)\n",
    "n_epoch = 5  # To be tuned \n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    autoencoder.train()\n",
    "    lie_operator.train()\n",
    "    total_train_loss = 0\n",
    "    total_loss1, total_loss2, total_loss3, total_loss4 = 0, 0, 0, 0\n",
    "    for tensor2d_batch_x, tensor2d_batch_x_next in train_dataloader:\n",
    "        tensor2d_batch_x = tensor2d_batch_x.to(device)\n",
    "        tensor2d_batch_x_next = tensor2d_batch_x_next.to(device)\n",
    "\n",
    "        optimiser_autoencoder.zero_grad()\n",
    "        optimiser_lie.zero_grad()\n",
    "\n",
    "        # dgX = lie_operator * gX\n",
    "        # jvp = \\nabla_x g (x) * f(x) (jvp: jacobian vector product)\n",
    "        (tensor2d_observable, tensor2d_jvp) = \\\n",
    "            autograd.functional.jvp(autoencoder.encoder,\n",
    "                                    tensor2d_batch_x,\n",
    "                                    tensor2d_batch_x_next,\n",
    "                                    create_graph=True)\n",
    "\n",
    "        tensor2d_decoded_x = autoencoder.decoder(tensor2d_observable)\n",
    "\n",
    "        tensor2d_lie_observable_next = lie_operator(tensor2d_observable)\n",
    "        tensor2d_predict_x_next = autoencoder.decoder(tensor2d_lie_observable_next)\n",
    "\n",
    "        tensor_loss_val = \\\n",
    "            loss(tensor2d_x=tensor2d_batch_x,\n",
    "                 tensor2d_x_next=tensor2d_batch_x_next,\n",
    "                 tensor2d_decoded_x=tensor2d_decoded_x,\n",
    "                 tensor2d_observable_next=tensor2d_observable,\n",
    "                 tensor2d_lie_observable_next=tensor2d_lie_observable_next,\n",
    "                 tensor2d_predict_x_next=tensor2d_predict_x_next,\n",
    "                 tensor2d_jvp=tensor2d_jvp)\n",
    "\n",
    "        tensor_loss_val.backward()\n",
    "        optimiser_autoencoder.step()\n",
    "        optimiser_lie.step()\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            total_train_loss += tensor_loss_val.item()\n",
    "    if epoch % 1 == 0:\n",
    "        print(epoch, total_train_loss)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        autoencoder.eval()\n",
    "        lie_operator.eval()\n",
    "        with torch.no_grad():\n",
    "            total_test_loss = 0\n",
    "            total_loss1, total_loss2, total_loss3, total_loss4 = 0, 0, 0, 0\n",
    "            for tensor2d_batch_x, tensor2d_batch_x_next in test_dataloader:\n",
    "                tensor2d_batch_x = tensor2d_batch_x.to(device)\n",
    "                tensor2d_batch_x_next = tensor2d_batch_x_next.to(device)\n",
    "\n",
    "                (tensor2d_observable, tensor2d_jvp) = \\\n",
    "                    autograd.functional.jvp(autoencoder.encoder,\n",
    "                                            tensor2d_batch_x,\n",
    "                                            tensor2d_batch_x_next,\n",
    "                                            create_graph=True)\n",
    "                tensor2d_decoded_x = autoencoder.decoder(tensor2d_observable)\n",
    "\n",
    "                tensor2d_lie_observable_next = lie_operator(tensor2d_observable)\n",
    "                tensor2d_predict_x_next = autoencoder.decoder(tensor2d_lie_observable_next)\n",
    "\n",
    "                tensor_loss_val = \\\n",
    "                    loss(tensor2d_x=tensor2d_batch_x,\n",
    "                         tensor2d_x_next=tensor2d_batch_x_next,\n",
    "                         tensor2d_decoded_x=tensor2d_decoded_x,\n",
    "                         tensor2d_observable_next=tensor2d_observable,\n",
    "                         tensor2d_lie_observable_next=tensor2d_lie_observable_next,\n",
    "                         tensor2d_predict_x_next=tensor2d_predict_x_next,\n",
    "                         tensor2d_jvp=tensor2d_jvp)\n",
    "\n",
    "                total_test_loss += tensor_loss_val.item()\n",
    "            print('-' * 50, 'TEST', '-' * 50)\n",
    "            print(epoch, total_test_loss)\n",
    "            print('-' * 106)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392e8467",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j"
   },
   "source": [
    "\n",
    "### Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f049c2e",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j"
   },
   "source": [
    "**Question 6.** : As in the **Question 3.** we want to ensure the Lie operator is stable. This can be verified by checking that the real part of the eigenvalues is negative. Plot the relevant eigenvalues of the Lie operator. You can use the [`numpy.linalg.eig`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html) function to retrieve the eigenvalues of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfffd5f",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:42.670396Z",
     "start_time": "2024-03-11T08:18:42.667662Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Check the eigenvalues real part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c6f676",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j"
   },
   "outputs": [],
   "source": [
    "# TODO: Plot the eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef7b9f6",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:42.787726Z",
     "start_time": "2024-03-11T08:18:42.672088Z"
    },
    "id": "jK7ZfTrBNILp"
   },
   "outputs": [],
   "source": [
    "n_grid = 30\n",
    "array_x1 = np.linspace(x1_min, x1_max, n_grid, dtype=np.float32)\n",
    "array_x2 = np.linspace(x2_min, x2_max, n_grid, dtype=np.float32)\n",
    "matrix_grid_x1, matrix_grid_x2 = np.meshgrid(array_x1, array_x2)\n",
    "\n",
    "array3d_dynamics = np.zeros((n_grid, n_grid, 2))\n",
    "\n",
    "for i in range(n_grid):\n",
    "    for j in range(n_grid):\n",
    "        array3d_dynamics[i, j, :] = duffing(np.array([matrix_grid_x1[i, j], matrix_grid_x2[i, j]]))\n",
    "\n",
    "autoencoder.eval()\n",
    "lie_operator.eval()\n",
    "array3d_dynamics_pred = np.zeros((n_grid, n_grid, 2))\n",
    "for i in range(n_grid):\n",
    "    for j in range(n_grid):\n",
    "        x1 = matrix_grid_x1[i, j]\n",
    "        x2 = matrix_grid_x2[i, j]\n",
    "        tensor2d_x = torch.tensor([[x1, x2]], dtype=torch.float32).to(device)\n",
    "        tensor2d_observable = autoencoder.encoder(tensor2d_x)\n",
    "        tensor2d_lie_observable_next = lie_operator(tensor2d_observable)\n",
    "        tensor2d_predict_x_next = autoencoder.decoder(tensor2d_lie_observable_next)\n",
    "        array_x_next = tensor2d_predict_x_next.cpu().detach().numpy().ravel()\n",
    "\n",
    "        array3d_dynamics_pred[i, j, :] = array_x_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4654add8",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j",
    "ExecuteTime": {
     "end_time": "2024-03-11T08:18:43.091730Z",
     "start_time": "2024-03-11T08:18:42.789957Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "DjQyXlTnQ4Pl",
    "outputId": "d40ba8f8-81c3-4007-be9e-805ef7655920"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax = fig.add_subplot(131)\n",
    "ax.quiver(matrix_grid_x1, matrix_grid_x2, array3d_dynamics[:, :, 0], array3d_dynamics[:, :, 1], scale=10)\n",
    "ax.set_title('True')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "ax.quiver(matrix_grid_x1, matrix_grid_x2, array3d_dynamics_pred[:, :, 0], array3d_dynamics_pred[:, :, 1], scale=10)\n",
    "ax.set_title('Prediction')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "cp = ax.contourf(matrix_grid_x1, matrix_grid_x2,\n",
    "                 np.log(np.linalg.norm(array3d_dynamics - array3d_dynamics_pred, axis=2)))\n",
    "fig.colorbar(cp)\n",
    "ax.set_title('Error in log scale')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d342ad6d",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j"
   },
   "source": [
    "**Question 7.** : Compare and comment below the two approaches (discrete vs continuous operator)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de65cee",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j"
   },
   "source": [
    "**Question 8.** : Mention one of the important research paper related to this framework. Namely, _deep learning of dynamical systems with the Koopman operator_. Explain shortly the reason of your choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199bbcaf",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "621e75ec",
   "metadata": {
    "cell_ktag": "G4CM001dJK9j"
   },
   "source": [
    "**-- PLEASE SUBMIT YOUR COMPLETED NOTEBOOK WITH CELL OUTPUTS --**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "eigen_penalty_stable.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kfiletag": "G4CM001dJK9j",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
